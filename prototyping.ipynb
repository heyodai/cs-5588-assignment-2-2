{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2-2: Building and Deploying the Prototype of the Triangle Model\n",
    "\n",
    "As part of Assignment 2, each student must build and deploy a triangle model prototype consisting of a frontend, a NoSQL database, and a backend Flask Python application. The Flask application must integrate the deep learning or LLM model you reproduced in Assignment 2-1 from your Jupyter Notebook. This task will establish a functional prototype that connects all three components and deploys the model. Up to two students from the same project team may collaborate on development, but each student must submit their own individual work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import ollama\n",
    "import gradio as gr\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1. Create the FastAPI app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI()\n",
    "\n",
    "\n",
    "class Query(BaseModel):\n",
    "    message: str\n",
    "\n",
    "\n",
    "@app.post(\"/query/\")\n",
    "async def query_model(query: Query):\n",
    "    response = \"This is where the model's response will go\"\n",
    "    return {\"response\": response}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2. Set up the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Ollama\n",
    "def query_model(query: Query):\n",
    "    response = ollama.chat(\n",
    "        model=\"llama3\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Why is the sky blue?\",\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    return response[\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3. Set up Gradio interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_model(message):\n",
    "    response = requests.post(\"http://localhost:8000/query/\", json={\"message\": message})\n",
    "    return response.json()[\"response\"]\n",
    "\n",
    "\n",
    "interface = gr.Interface(\n",
    "    fn=chat_with_model,\n",
    "    inputs=\"text\",\n",
    "    outputs=\"text\",\n",
    "    title=\"Chat with Model\",\n",
    "    description=\"Type a message to get a response from the model.\",\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    interface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
